{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "SNOW_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Snow\")\n",
    "NOT_SNOW_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Not_Snow\")\n",
    "OUT_MASK_DIR = SNOW_DIR.parent / \"Anomaly_Masks\"\n",
    "OUT_MASK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BANDS = 12\n",
    "PERCENTILE = 95  # For thresholding\n",
    "EPOCHS = 40\n",
    "BATCH = 8192\n",
    "LATENT_DIM = 8\n",
    "THRESHOLD_METHOD = 'percentile'  # or 'mean+2std'\n",
    "\n",
    "# === HELPERS ===\n",
    "def read_all_pixels_from_folder(folder, max_images=100):\n",
    "    \"\"\"Reads all pixels from all .tif files in a folder, returns a (N, BANDS) array.\"\"\"\n",
    "    pixel_list = []\n",
    "    for i, tif in enumerate(sorted(folder.glob(\"*.tif\"))):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        with rasterio.open(tif) as src:\n",
    "            arr = src.read(list(range(1, BANDS+1))).astype('float32') / 10000.0  # (12, H, W)\n",
    "            arr = arr.reshape(BANDS, -1).T  # (H*W, 12)\n",
    "            nodata = src.nodata\n",
    "            if nodata is not None:\n",
    "                mask = np.any(arr == nodata, axis=1)\n",
    "                arr = arr[~mask]\n",
    "            arr = arr[~np.isnan(arr).any(axis=1)]\n",
    "            pixel_list.append(arr)\n",
    "    if pixel_list:\n",
    "        X = np.vstack(pixel_list)\n",
    "    else:\n",
    "        X = np.empty((0, BANDS))\n",
    "    return X\n",
    "\n",
    "# === 1. LOAD DATA ===\n",
    "print(\"Loading Snow spectra...\")\n",
    "X_snow = read_all_pixels_from_folder(SNOW_DIR)\n",
    "print(f\"  Got {X_snow.shape[0]:,} pixels\")\n",
    "\n",
    "print(\"Loading Not Snow spectra...\")\n",
    "X_not_snow = read_all_pixels_from_folder(NOT_SNOW_DIR)\n",
    "print(f\"  Got {X_not_snow.shape[0]:,} pixels\")\n",
    "\n",
    "# === 2. PREPROCESSING ===\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_snow_imp = imputer.fit_transform(X_snow)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_snow_scaled = scaler.fit_transform(X_snow_imp)\n",
    "\n",
    "# === 3. BUILD AUTOENCODER ===\n",
    "input_layer = layers.Input(shape=(BANDS,))\n",
    "x = layers.Dense(32, activation=\"relu\")(input_layer)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "encoded = layers.Dense(LATENT_DIM, activation=\"relu\")(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(encoded)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "decoded = layers.Dense(BANDS, activation=\"linear\")(x)\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# === 4. TRAIN AUTOENCODER ON SNOW ONLY ===\n",
    "autoencoder.fit(X_snow_scaled, X_snow_scaled,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH,\n",
    "                shuffle=True,\n",
    "                validation_split=0.05,\n",
    "                verbose=2,\n",
    "                callbacks=[callbacks.EarlyStopping(patience=4, restore_best_weights=True)])\n",
    "\n",
    "# === 5. GET RECONSTRUCTION ERROR DISTRIBUTION (ON SNOW) ===\n",
    "recon_snow = autoencoder.predict(X_snow_scaled, batch_size=BATCH, verbose=1)\n",
    "err_snow = np.mean((recon_snow - X_snow_scaled) ** 2, axis=1)\n",
    "if THRESHOLD_METHOD == 'percentile':\n",
    "    thr = np.percentile(err_snow, PERCENTILE)\n",
    "elif THRESHOLD_METHOD == 'mean+2std':\n",
    "    thr = err_snow.mean() + 2*err_snow.std()\n",
    "print(f\"Anomaly threshold (95th percentile): {thr:.6g}\")\n",
    "\n",
    "# === 6. FUNCTION TO GENERATE MASK FOR AN IMAGE ===\n",
    "def make_anomaly_mask(tif_path):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        arr = src.read(list(range(1, BANDS+1))).astype('float32') / 10000.0  # (12, H, W)\n",
    "        h, w = arr.shape[1], arr.shape[2]\n",
    "        arr = arr.reshape(BANDS, -1).T  # (H*W, 12)\n",
    "        nodata = src.nodata\n",
    "        mask_valid = np.ones(arr.shape[0], dtype=bool)\n",
    "        if nodata is not None:\n",
    "            mask_valid = ~np.any(arr == nodata, axis=1)\n",
    "        arr_imp = imputer.transform(arr)\n",
    "        arr_scaled = scaler.transform(arr_imp)\n",
    "        recon = autoencoder.predict(arr_scaled, batch_size=8192, verbose=0)\n",
    "        err = np.mean((recon - arr_scaled)**2, axis=1)\n",
    "        mask = np.zeros(arr.shape[0], dtype=np.uint8)\n",
    "        mask[mask_valid] = (err[mask_valid] < thr).astype(np.uint8)\n",
    "        mask_img = mask.reshape(h, w)\n",
    "        return mask_img, src.profile\n",
    "\n",
    "# === 7. GENERATE MASKS FOR ALL IMAGES AND SAVE ===\n",
    "def process_folder(in_folder, out_folder):\n",
    "    for tif in tqdm(sorted(in_folder.glob(\"*.tif\"))):\n",
    "        mask_img, prof = make_anomaly_mask(tif)\n",
    "        prof.update(count=1, dtype='uint8', nodata=0, compress='lzw')\n",
    "        out_path = out_folder / (tif.stem + \"_mask.tif\")\n",
    "        with rasterio.open(out_path, \"w\", **prof) as dst:\n",
    "            dst.write(mask_img, 1)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(\"Generating masks for Snow images...\")\n",
    "process_folder(SNOW_DIR, OUT_MASK_DIR)\n",
    "print(\"Generating masks for Not Snow images...\")\n",
    "process_folder(NOT_SNOW_DIR, OUT_MASK_DIR)\n",
    "\n",
    "# === 8. SAVE MODEL AND PREPROCESSORS ===\n",
    "autoencoder.save(str(OUT_MASK_DIR / \"snow_autoencoder.h5\"))\n",
    "joblib.dump({\"imputer\": imputer, \"scaler\": scaler, \"thr\": thr}, OUT_MASK_DIR / \"snow_ae_preprocessing.pkl\")\n",
    "print(\"✓ All done! Masks, model, and preprocessors saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42cac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === PATHS ===\n",
    "SNOW_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Snow\")\n",
    "NOT_SNOW_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Not_Snow\")\n",
    "OUT_SCORE_DIR = SNOW_DIR.parent / \"Anomaly_Score_Maps_V2\"\n",
    "OUT_SCORE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_PATH = SNOW_DIR.parent / \"Anomaly_Masks\" / \"snow_autoencoder.h5\"\n",
    "PREP_PATH = SNOW_DIR.parent / \"Anomaly_Masks\" / \"snow_ae_preprocessing.pkl\"\n",
    "\n",
    "BANDS = 12\n",
    "\n",
    "# === LOAD MODEL AND PREPROCESSORS ===\n",
    "autoencoder = load_model(MODEL_PATH)\n",
    "prep = joblib.load(PREP_PATH)\n",
    "imputer = prep[\"imputer\"]\n",
    "scaler = prep[\"scaler\"]\n",
    "\n",
    "# For score normalization:\n",
    "if \"err_snow_sorted\" in prep:\n",
    "    err_snow_sorted = prep[\"err_snow_sorted\"]\n",
    "    N_h = len(err_snow_sorted)\n",
    "else:\n",
    "    # Recompute using Snow pixels \n",
    "    print(\"No err_snow_sorted found. Will recompute on the fly if needed.\")\n",
    "\n",
    "# === COMPUTE CDF FOR ERROR (FROM TRAIN SNOW PIXELS) ===\n",
    "\n",
    "def anomaly_score_from_error(err, err_snow_sorted):\n",
    "    idx = np.searchsorted(err_snow_sorted, err, side=\"right\")\n",
    "    score = np.clip(idx / len(err_snow_sorted), 0, 1)\n",
    "    return score\n",
    "\n",
    "# === SCORE MAP FUNCTION ===\n",
    "def make_anomaly_score_map(tif_path, err_snow_sorted):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        arr = src.read(list(range(1, BANDS+1))).astype('float32') / 10000.0  # (12, H, W)\n",
    "        h, w = arr.shape[1], arr.shape[2]\n",
    "        arr = arr.reshape(BANDS, -1).T  # (H*W, 12)\n",
    "        nodata = src.nodata\n",
    "        mask_valid = np.ones(arr.shape[0], dtype=bool)\n",
    "        if nodata is not None:\n",
    "            mask_valid = ~np.any(arr == nodata, axis=1)\n",
    "        arr_imp = imputer.transform(arr)\n",
    "        arr_scaled = scaler.transform(arr_imp)\n",
    "        recon = autoencoder.predict(arr_scaled, batch_size=8192, verbose=0)\n",
    "        err = np.mean((recon - arr_scaled)**2, axis=1)\n",
    "        score = np.zeros(arr.shape[0], dtype=np.float32)\n",
    "        score[mask_valid] = anomaly_score_from_error(err[mask_valid], err_snow_sorted)\n",
    "        score_img = score.reshape(h, w)\n",
    "        return score_img, src.profile\n",
    "\n",
    "# === LOAD SNOW PIXELS TO GET err_snow_sorted ===\n",
    "def read_all_pixels_from_folder(folder, max_images=100):\n",
    "    pixel_list = []\n",
    "    for i, tif in enumerate(sorted(folder.glob(\"*.tif\"))):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        with rasterio.open(tif) as src:\n",
    "            arr = src.read(list(range(1, BANDS+1))).astype('float32') / 10000.0  # (12, H, W)\n",
    "            arr = arr.reshape(BANDS, -1).T  # (H*W, 12)\n",
    "            nodata = src.nodata\n",
    "            if nodata is not None:\n",
    "                mask = np.any(arr == nodata, axis=1)\n",
    "                arr = arr[~mask]\n",
    "            arr = arr[~np.isnan(arr).any(axis=1)]\n",
    "            pixel_list.append(arr)\n",
    "    if pixel_list:\n",
    "        X = np.vstack(pixel_list)\n",
    "    else:\n",
    "        X = np.empty((0, BANDS))\n",
    "    return X\n",
    "\n",
    "print(\"Building snow error CDF for scoring...\")\n",
    "X_snow = read_all_pixels_from_folder(SNOW_DIR)\n",
    "X_snow_imp = imputer.transform(X_snow)\n",
    "X_snow_scaled = scaler.transform(X_snow_imp)\n",
    "recon_snow = autoencoder.predict(X_snow_scaled, batch_size=8192, verbose=1)\n",
    "err_snow = np.mean((recon_snow - X_snow_scaled) ** 2, axis=1)\n",
    "err_snow_sorted = np.sort(err_snow)\n",
    "\n",
    "# === PROCESS FOLDERS ===\n",
    "def process_folder_score(in_folder, out_folder):\n",
    "    for tif in tqdm(sorted(in_folder.glob(\"*.tif\"))):\n",
    "        score_img, prof = make_anomaly_score_map(tif, err_snow_sorted)\n",
    "        prof.update(count=1, dtype='float32', nodata=np.nan, compress='lzw')\n",
    "        out_path = out_folder / (tif.stem + \"_score.tif\")\n",
    "        with rasterio.open(out_path, \"w\", **prof) as dst:\n",
    "            dst.write(score_img, 1)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(\"Generating continuous anomaly score maps for Snow images...\")\n",
    "process_folder_score(SNOW_DIR, OUT_SCORE_DIR)\n",
    "print(\"Generating continuous anomaly score maps for Not Snow images...\")\n",
    "process_folder_score(NOT_SNOW_DIR, OUT_SCORE_DIR)\n",
    "\n",
    "print(\"✓ All done! Continuous anomaly score maps saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === PATHS ===\n",
    "SNOW_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Snow\")\n",
    "NOT_SNOW_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Not_Snow\")\n",
    "SCORE_MAPS_DIR = Path(r\"C:\\mnarimani\\1-UCDavis\\16-ESEARCH\\2-Clean_Data\\Anomaly_Score_Maps\")\n",
    "\n",
    "# --- RENAME SNOW FILES ---\n",
    "snow_files = sorted(SNOW_DIR.glob(\"Square_*.tif\"))\n",
    "for idx, file in enumerate(snow_files, 1):\n",
    "    new_name = f\"Snow_{idx}.tif\"\n",
    "    new_path = file.with_name(new_name)\n",
    "    print(f\"Renaming {file.name} -> {new_name}\")\n",
    "    file.rename(new_path)\n",
    "\n",
    "# --- RENAME NOT SNOW FILES ---\n",
    "not_snow_files = sorted(NOT_SNOW_DIR.glob(\"Square_*.tif\"))\n",
    "for idx, file in enumerate(not_snow_files, 1):\n",
    "    new_name = f\"Not_Snow_{idx}.tif\"\n",
    "    new_path = file.with_name(new_name)\n",
    "    print(f\"Renaming {file.name} -> {new_name}\")\n",
    "    file.rename(new_path)\n",
    "\n",
    "# --- RENAME ANOMALY SCORE MAPS (MATCHING TO SNOW AND NOT SNOW) ---\n",
    "\n",
    "# For Snow\n",
    "snow_rename_map = {}\n",
    "for idx, old_file in enumerate(sorted(SCORE_MAPS_DIR.glob(\"Square*_score.tif\")), 1):\n",
    "    # Check if original file is in SNOW_DIR\n",
    "    orig_file = SNOW_DIR.parent / \"Snow\" / f\"Snow_{idx}.tif\"\n",
    "    new_score_name = f\"Snow_{idx}_Score.tif\"\n",
    "    snow_rename_map[old_file.name] = new_score_name\n",
    "\n",
    "# For Not Snow\n",
    "not_snow_rename_map = {}\n",
    "for idx, old_file in enumerate(sorted(SCORE_MAPS_DIR.glob(\"Square*_score.tif\")), 1):\n",
    "    # Check if original file is in NOT_SNOW_DIR\n",
    "    orig_file = NOT_SNOW_DIR.parent / \"Not_Snow\" / f\"Not_Snow_{idx}.tif\"\n",
    "    new_score_name = f\"Not_Snow_{idx}_Score.tif\"\n",
    "    not_snow_rename_map[old_file.name] = new_score_name\n",
    "\n",
    "# Now rename all score maps, using the presence of a file in the original directory as the criterion.\n",
    "score_files = sorted(SCORE_MAPS_DIR.glob(\"Square*_score.tif\"))\n",
    "for idx, score_file in enumerate(score_files, 1):\n",
    "    # Try Snow first:\n",
    "    if idx <= len(snow_files):\n",
    "        new_score_name = f\"Snow_{idx}_Score.tif\"\n",
    "    else:\n",
    "        new_score_name = f\"Not_Snow_{idx - len(snow_files)}_Score.tif\"\n",
    "    new_score_path = score_file.with_name(new_score_name)\n",
    "    print(f\"Renaming {score_file.name} -> {new_score_name}\")\n",
    "    score_file.rename(new_score_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
